
**定义**：KL散度就是用猜测 q 来描述真是情况 p 时会损失多少信息。

- KL = 0，无信息损失
- KL > 0 ，信息损失量


**数学原理**：

- $P(x) -$ 真实分布， $Q(x)-$近似分布，$log\frac{P(x)}{Q(x)}-$在点 x 处两个分布得偏离程度
- 离散型随机变量：$D_{KL}(P||Q) = \Sigma_x P(x)log\frac{P(x)}{Q(x)}$
- 连续性随机变量：$D_{KL}(P||Q) = \int P(x)log\frac{P(x)}{Q(x)}dx$
- 不难看出，**在高概率区域，差异会被放大**（P(x)系数）

- **非负性**，**利用Jesen不等式证明**
- **可加性**，$D_{KL}(P_1P_2||Q_1Q_2) = D_{KL}(P_1||Q_1) + D_{KL}(P_2||Q_2)$ 
	- 在高维控件，可逐维拆解分析误差来源
	- 分层模型涉及中，可以分别衡量每层得偏离
- **不对称性**，$D_{KL}(P||Q) \neq D_{KL}(Q||P)$
	- 不能用KL散度当作 “**距离函数**” 使用 -- 不对成所以不是距离
	- 用P来逼近Q和用Q来逼近P代价不一样

**信息论中最优编码长度**：对于概率为 p 得事件，最优编码长度是 $-log_2(p)$ bits

- 晴天（50%）：$-log_2(0.5) = 1$ bit
- 阴天（25%）：$-log_2(0.25) = 2$ bits

**交叉熵**（用Q编码P）：$H(P,Q) = -\Sigma P(x)log_2Q(x)$

**熵**（最优编码）：$H(p) = -\Sigma P(x) \cdot log_2P(x)$

**KL散度**：$KL(P||Q) = H(P,Q) - H(P) = \Sigma P(x)log_2(P(x)/Q(x))$

 **KL散度是用来度量使用基于Q的分布来编码服从P的分布的样本所需的额外的平均比特数**
