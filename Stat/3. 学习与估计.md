
### KL散度：最大似然估计的底层逻辑


**衡量两个概率分布差异的尺子**：相对熵 - KL散度

1. **假定数据是按以下方式生成**：$Y_i$ ~ $G(y), i.i.d. i = 1,...,n$

- $G$ 代表某种一般性的概率分布
- $G(y)$ 代表在取值 $y$ 处的**累积概率**
- $Y_i$ 代表不同观测对象的随机变量，符号 $Y(\omega_1)$ **书写上** 简化为 $Y_1$，类似 $Y(张三)$ = 165 （获取身高）
- $y_i$ 代表观察值
- 那么 $y_i = Y(张三）= Y(\omega_1) = Y_1 = 165$
	- 165 - 观测到的值，也就是随机变量映射后的值
	- $Y_1$ - 统计学简化的符号，第一个随机变量
	- $Y(\omega_1)$ - 随机变量 $Y$ 作用在第一个样本点 $\omega_1$ 上
	- $y_1$ - 观测到的具体数值
	- 张三 - 第一个样本点
- $Y_i$ ~ $G(y) \quad i.i.d. \quad i = 1,...,n$ 意思就是按照 G 分布，抽取了 $n$ 个样本
	
2. **模型逼近**

- 由于分布（数据生成过程） $G$ 是未知的，我们想用一个模型来 “**逼近**” 他
- $\mathcal{F} = \{F(\cdot;\theta),\theta \in \Theta\}$
- $F(\cdot;\theta)$ - 一个分布函数，$\theta$ 是分布函数的参数，例如 指数分布中的 $\lambda$ 或者正态分布里面的 $(\mu,\sigma)$
- $\Theta$ - 参数空间，是所有可能的参数值的集合
- $\mathcal{F}$ - 一个集合，包括所有可能的分布函数
- 目标是在集合 $\mathcal{F}$ 内找到 **真实** 但 **未知** 的数据生成过程 G(·)的 **最佳近似**
	
 3. **KL散度**

- **定义**：$KL(G,F) = \int log \frac{g(y)}{f(y)} dG(y)$
	- $f(y),g(y)$ - 概率或者密度函数
	- 如果 分布是连续的，$KL(G,F) = \int log \frac{g(y)}{f(y)} \cdot g(y) = \int log(g(y)) \cdot g(y)dy - \int log(f(y)) \cdot g(y)dy$
	- **熵**：$\int log(g(y)) \cdot g(y)dy$
- **非对称**：$KL(G,F) \neq KL(F,G)$
- $\theta_0$ - 使得 $KL(G,F)$ 散度最小的参数值，也成为 “ **真实参数** ”
- $\hat{\theta}$ - 参数的 **估计值**
- **估计分布** - $F(\cdot;\hat{\theta})$
- **理论最优分布** - $F(\cdot;\theta_0)$

4. **求满足满足散度最低的参数：**

- $KL(G,F) = \int log \frac{g(y)}{f(y;\theta)} dG(y)$
- **我们的目标**：找到使KL散度最小的参数θ

- **微积分中的最优化原理**：

- 想象KL散度是关于 θ 的一个曲线
- **一阶必要条件**：如果 $\theta_0$是最小值点，则必有 $\frac{\partial}{\partial \theta}KL(G,F(\cdot;\theta)|_{\theta = \theta_0} = 0$（**注意！反之不一定**）
- **解方程，可能存在唯一解、多个解、无解三种场景。**
$$
\begin{align}
0 &= \frac{\partial}{\partial\theta} \int \log \frac{g(y)}{f(y;\theta)} dG(y) \\[10pt]
&= \frac{\partial}{\partial\theta} \int [\log g(y) - \log f(y;\theta)] dG(y) 
\quad \color{blue}{\text{[对数性质]}} \\[10pt]
&= \frac{\partial}{\partial\theta}\int \log g(y) dG(y) - \frac{\partial}{\partial\theta}\int \log f(y;\theta) dG(y) 
\quad \color{blue}{\text{[积分线性性]}} \\[10pt]
&= 0 - \frac{\partial}{\partial\theta}\int \log f(y;\theta) dG(y) 
\quad \color{blue}{\text{[常数导数为0]}} \\[10pt]
&= -\int \frac{\partial}{\partial\theta}\log f(y;\theta) dG(y) 
\quad \color{blue}{\text{[Leibniz法则]}}
\end{align}
$$
- **即**：最小化KL散度 $\min_\theta KL(G, F(\cdot;\theta))$ 等价于求解 $\mathbb{E}\left[\frac{\partial}{\partial\theta}\log f(Y;\theta)\right] = 0$
- 应当注意的是,上述积分在实践中 **无法计算**, **因为分布 G(·) 是未知的**
- 然而,我们有通过 G(·) 生成的数据 $y_1, ..., y_n$，对于任何可测函数(统计量) $t(Y)$ ,这意味着：

	**大数定理的一般形式**：$\frac{1}{n}\sum_{i=1}^{n} t(y_i) \xrightarrow{n\to\infty} E(t(Y))$

- 根据以上大数定理，KL散度可以通过以下方式 “**估计**”
$\frac{1}{n}\sum_{i=1}^{n} \log \frac{g(y_i)}{f(y_i;\theta)} = \frac{1}{n}\sum_{i=1}^{n} \log g(y_i) - \frac{1}{n}\sum_{i=1}^{n} \log f(y_i;\theta) \xrightarrow{n\to\infty} \int \log g(y) \cdot g(y)dy - \int \log f(y;\theta) \cdot g(y)dy.$
	**注意**：$g(y)$ 是随机变量 Y 的密度函数

- $g(y)$ 是真实密度函数，我们不知道真实密度，我们只有观测数据 $y_1,...,y_n$
- $f(y; \theta)$ 是我们 **指定的模型**（参数化的密度函数），对于给定的参数 $\theta$ 和观测值 $y_i$，可以**直接计算** $f(y_i; \theta)$
	- 真实分布 $g(y)$ 是**未知且无限复杂**的，我们永远无法知道真实的数据生成过程
	- 必须用**简化的模型** $f(y;\theta)$ 来近似
	- **统计建模的本质** 就是：**用一个错误但有用的模型来近似复杂的现实**
	- 统计不是在寻找"真理"，而是在有限信息下做**最优决策**
	- **所以**：我们可以 **假设一个分布**，例如正态分布，然后通过 **真实分布的采样值** 和 **不断尝试的参数**，来 **计算KL散度** 来 **逼近真实模型**
	
    真实世界（未知）                          我们的建模
─────────────────        ─────────────────
g(y) = 真实分布    →                     假设: f(y;θ) = N(μ,σ²)
	 ↓ 生成数据                                           ↓
	**观测样本**:                                      **尝试不同参数**:
	y₁, y₂, ..., yₙ                                      θ₁ = (μ₁,σ₁²)
                                 θ₂ = (μ₂,σ₂²)
                                 θ₃ = (μ₃,σ₃²)
                                 ...
         ↓
  **计算 KL 散度的"可估计部分"**
         ↓
  **找到最佳参数** $\hat{\theta}$


5. **似然函数定义：**$l(\theta) = \sum_{i=1}^{n} \log f(y_i;\theta)$，似然函数 **依赖于观测值** $y_1, ..., y_n$

	形式上可以写为：$l(\theta) = l(\theta;y_1,...,y_n)$

