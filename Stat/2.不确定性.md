
**不确定性来源：**

- **偶然不确定性**（aleatoric）：

	是世界本身的随机性导致 - 本质

- **认知不确定性**（epistemic）：

	我们认知不够导致 - 短板

一般来说，我们将使用概率模型，这将使我们能够**量化知识**，反过来也能**量化剩余的不确定性**。


### 不确定性的核心定义与区分

- 偶然不确定性 (Aleatoric Uncertainty)：源于事件固有的随机性，如由概率模型 `Y ~ G(·|x)` 所定义，原则上不可通过增加数据或知识来消除，是不可约的。
- 认知不确定性 (Epistemic Uncertainty)：源于知识缺乏，例如对模型 `G(·|x)` 的无知或数据量有限，原则上可通过增加数据量或进行更多实验来减少。
- 区分复杂性：两者之间的明确区分是困难的，且会根据可用的数据和知识状态（如骰子杯示例）而变化。

### 实践考量与研究前沿

- 总不确定性分解：普遍认为总不确定性分解为偶然不确定性和认知不确定性，但不一定是数学上的加法方式。
- 估计不确定性：由于数据的有限性，任何通过数据训练的模型都带有固有的估计不确定性。
- 活跃研究领域：对不确定性的理解、量化及其整合到现代机器学习工具中，仍是一个活跃的研究领域（如Gawlikowski等人2023）。
- 应用场景差异：在某些机器学习任务（如猫狗分类）中，偶然不确定性可能次要，但在医学图像分类等领域则非常关键。

$Y$ ~ $G(\cdot | x)$ - 数据生成过程，偶然不确定性的定义

- $G(\cdot | x)$ - 某个依赖 x 的随机分布函数
- 只要条件分布不是退化的，我们就不能无误差地从x预测y
- 这里的非退化意味着分布不集中于单个值y0
- 简单地说，假设方差存在，偶然不确定性是指严格正的方差Var(Y|x)


认知不确定性转化为模型不确定性和估计不确定性

**传统统计学**

- 我们如何通过有限的数据来了解一个更大的整体

- 在现有信息下，那个真实答案最可能在哪个范围里，以及我们对这个范围有多大的信心

- 第一，统计学就是用样本去推断总体；
- 第二，样本本身的平均值和离散程度是关键信息；
- 第三，也是最重要的，是用置信区间这样的工具去量化不确定性

样本集合：t(y)
总体：$\theta$
样本到总体：**推断**


**数据生成过程：未知分布G(·)如何驱动统计推断**

- 首先，我们所有的数据，都假设是从一个未知的概率分布G(·)中独立同分布抽取的
- 其次，虽然这个分布是未知的，但我们完全可以通过手头有限的数据样本去推断它
- 然后，靠的就是统计学的工具，比如经验分布函数，它会随着样本量的增加而收敛到真实的分布
- 就是统计学的工具，比如经验分布函数，它会随着样本量的增加而收敛到真实的分布。最后，大数定律这样的工具，能让我们从样本中估计出这个未知分布的重要特征，比如均值和方差

$Y$ ~ $G(\cdot)$ - 数据产生过程

**统计建模：用假设模型逼近现实，与机器学习异曲同工**


统计建模就是用一个我们设定好的数学模型，来尽可能准确地描述和估计真实世界


这种“数据越多，结果越准”的特性，我们称之为“一致性”

- 第一，我们承认现实世界是复杂的，我们用一个带参数的、已知的模型F去近似它。
- 第二，我们通过一个学习算法，从数据中去“学习”出这套模型里最优的参数。
- 第三，也是最重要的一点，我们选择的这个模型，它和机器学习里的损失函数是深度绑定的，它本身就是一个假设

### 统计建模的基本概念与组成

- 通过参数分布函数F(·; θ)近似未知的数据生成过程G(·)，其中θ是参数向量。
- 统计模型F被定义为一组分布函数集合，假设数据由F中某一元素针对特定参数值生成。
- F的选择类似于机器学习中损失函数的选择，其本质上是假设性的。

### 参数估计与学习算法

- 目标是以数据驱动的方式，以某种最优方式选择参数θ，得到估计值θ̂。
- θ̂是数据的某个函数（统计量或估计量），通常是估计算法t(·)的结果。
- 在机器学习中，此估计算法t(·)被称为学习算法。

### 模型假设的通用性与局限性

- 经典统计学常假设真实数据生成过程G(·)来自所选模型F (即G(·) = F(·, θ₀))。
- 原文强调F仅是一个用于近似现实的模型类，真实数据生成过程G(·)通常未知且可能与F不同。
- 若模型假设成立，则期望估计量θ̂随着数据量n的增加而趋近“真实”参数θ₀（一致性）。

### 参数优化的方法

- 建议通过优化某个目标函数来导出参数估计值θ̂。
- 在统计学中，**似然函数**是达到“**黄金标准**”的目标函数之一。


**参数分布函数**：$F(\cdot ; \theta), \theta = (\theta_1,...,\theta_n)^T$ 来近似 $G(\cdot)$

$t(\cdot)$ 是某种算法，算法再数据$y_1,....,y_n$ 的驱动下，产生 $\hat{\theta}$，


统计建模可以被视为选择适当的分布模型来解释偶然不确定性，类似于在机器学习中选择适当的损失函数






